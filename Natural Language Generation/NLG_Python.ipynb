{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from get_vocabulary import get_vocabulary\n",
    "from get_max_len import get_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>William</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>James</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>George</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     input\n",
       "0     John\n",
       "1  William\n",
       "2    James\n",
       "3  Charles\n",
       "4   George"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_df = pd.read_csv('data/names.txt', sep=\"\\n\", header=None)\n",
    "names_df.columns=['input']\n",
    "names_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess names dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\tJohn</td>\n",
       "      <td>John\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\tWilliam</td>\n",
       "      <td>William\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tJames</td>\n",
       "      <td>James\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tCharles</td>\n",
       "      <td>Charles\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\tGeorge</td>\n",
       "      <td>George\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       input     target\n",
       "0     \\tJohn     John\\n\n",
       "1  \\tWilliam  William\\n\n",
       "2    \\tJames    James\\n\n",
       "3  \\tCharles  Charles\\n\n",
       "4   \\tGeorge   George\\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert a tab in front of all the names\n",
    "names_df['input'] = names_df['input'].apply(lambda x : '\\t' + x)\n",
    "\n",
    "# Append a newline at the end of every name\n",
    "# We already appended a tab in front, so the target word should start at index 1\n",
    "names_df['target'] = names_df['input'].apply(lambda x : x[1:len(x)] + '\\n')\n",
    "names_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have a DataFrame with two columns containing the names with the start and end tokens appended. The next step is to encode these as numeric values because machine learning models only accept numeric inputs.\n",
    "- create two dictionaries, char_to_idx and idx_to_char, that will contain mappings of characters to integers, e.g., {'\\t': 0, '\\n': 1, 'a': 2, 'b': 3, ...} and the reverse mappings of integers to characters, e.g, {0: '\\t', 1: '\\n', 2: 'a', 3: 'b', ...}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 0, '\\n': 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8, 'H': 9, 'I': 10, 'J': 11, 'K': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'W': 24, 'X': 25, 'Y': 26, 'Z': 27, 'a': 28, 'b': 29, 'c': 30, 'd': 31, 'e': 32, 'f': 33, 'g': 34, 'h': 35, 'i': 36, 'j': 37, 'k': 38, 'l': 39, 'm': 40, 'n': 41, 'o': 42, 'p': 43, 'q': 44, 'r': 45, 's': 46, 't': 47, 'u': 48, 'v': 49, 'w': 50, 'x': 51, 'y': 52, 'z': 53}\n",
      "{0: '\\t', 1: '\\n', 2: 'A', 3: 'B', 4: 'C', 5: 'D', 6: 'E', 7: 'F', 8: 'G', 9: 'H', 10: 'I', 11: 'J', 12: 'K', 13: 'L', 14: 'M', 15: 'N', 16: 'O', 17: 'P', 18: 'Q', 19: 'R', 20: 'S', 21: 'T', 22: 'U', 23: 'V', 24: 'W', 25: 'X', 26: 'Y', 27: 'Z', 28: 'a', 29: 'b', 30: 'c', 31: 'd', 32: 'e', 33: 'f', 34: 'g', 35: 'h', 36: 'i', 37: 'j', 38: 'k', 39: 'l', 40: 'm', 41: 'n', 42: 'o', 43: 'p', 44: 'q', 45: 'r', 46: 's', 47: 't', 48: 'u', 49: 'v', 50: 'w', 51: 'x', 52: 'y', 53: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary\n",
    "vocabulary = get_vocabulary(names_df['input'])\n",
    "\n",
    "# Sort the vocabulary\n",
    "vocabulary_sorted = sorted(vocabulary)\n",
    "\n",
    "# Create the mapping of the vocabulary chars to integers\n",
    "char_to_idx = { char : idx for idx, char in enumerate(vocabulary_sorted) }\n",
    "\n",
    "# Create the mapping of the integers to vocabulary chars\n",
    "idx_to_char = { idx : char for idx, char in enumerate(vocabulary_sorted) }\n",
    "\n",
    "# Print the dictionaries\n",
    "print(char_to_idx)\n",
    "print(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T', 'R', 'C', 'n', 'p', 't', 'x', 'N', 'J', 'd', 'o', '\\n', 'S', 'z', 'V', 'E', 'm', 'y', 'D', 'c', 'W', 'I', 'F', 'X', 'b', 'v', 'r', 'Q', 'L', 's', 'h', 'G', 'O', 'B', 'j', '\\t', 'M', 'k', 'U', 'A', 'u', 'l', 'P', 'Z', 'g', 'q', 'i', 'f', 'K', 'Y', 'w', 'e', 'H', 'a'}\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input and target tensors\n",
    "- The input is a list containing all the names in the dataset. So, the first dimension of the input tensor will be the number of names in the dataset. Each name can be thought of as a string having length equal to the length of the longest name and each character in each name is a one-hot encoded vector of size vocabulary. So, the second and third dimensions of the input tensor will be the length of the longest name and the size of the vocabulary. Similar is the case for the target tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(258000, 13, 54) (258000, 13, 54) 12\n"
     ]
    }
   ],
   "source": [
    "# Find the length of longest name\n",
    "max_len = get_max_len(names_df['input'])\n",
    "\n",
    "# Initialize the input vector\n",
    "input_data = np.zeros((len(names_df['input']), max_len+1, len(vocabulary)), dtype='float32')\n",
    "\n",
    "# Initialize the target vector\n",
    "target_data = np.zeros((len(names_df['input']), max_len+1, len(vocabulary)), dtype='float32')\n",
    "\n",
    "print(input_data.shape, target_data.shape, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we have two vectors of appropriate shape which we can fill up with actual data. These vectors can then be fed to the recurrent neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize input and target vectors with values\n",
    "- We created the input and target tensors of appropriate shape containing all zeros. Now, we'll fill these with actual values. The input and target tensors contain all the names in the dataset. Each name can be thought of as a string having length equal to the length of the longest name and each character in each name is a one-hot encoded vector of size vocabulary.\n",
    "- The tensors can be filled-in as follows: `input_data[n_idx, p_idx, char_to_idx[char]]` will be set to 1 whenever the index of the name in the dataset is `n_idx` and it contains the character char in position `p_idx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3-d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((11,12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1,1,10] = 1\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate for each name in the dataset\n",
    "for n_idx, name in enumerate(names_df['input']):\n",
    "  # Iterate over each character and convert it to a one-hot encoded vector\n",
    "  for c_idx, char in enumerate(name):\n",
    "    input_data[n_idx, c_idx, char_to_idx[char]] = 1\n",
    "\n",
    "# Iterate for each name in the dataset\n",
    "for n_idx, name in enumerate(names_df['target']):\n",
    "  # Iterate over each character and convert it to a one-hot encoded vector\n",
    "  for c_idx, char in enumerate(name):\n",
    "    target_data[n_idx, c_idx, char_to_idx[char]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we have the input and target vectors of appropriate shape. We can use these vectors to train the recurrent neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and compile RNN network\n",
    "- We completed all the data preprocessing steps and have the input and target vectors ready. It is time to build the recurrent neural network. We'll create a small network architecture that will have 50 simple RNN nodes in the first layer followed by a dense layer. The dense layer will generate a probability distribution over the vocabulary for the next character. So, the size of the dense layer will be the same as the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, SimpleRNN, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 13, 50)            5250      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 13, 54)            2754      \n",
      "=================================================================\n",
      "Total params: 8,004\n",
      "Trainable params: 8,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add SimpleRNN layer of 50 units\n",
    "model.add(SimpleRNN(50, input_shape=(max_len+1, len(vocabulary)), return_sequences=True))\n",
    "\n",
    "# Add a TimeDistributed Dense layer of size same as the vocabulary\n",
    "model.add(TimeDistributed(Dense(len(vocabulary), activation='softmax')))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Built and compiled the recurrent neural network model successfully! This model can be trained now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN model and start predictions\n",
    "- The output name will be generated character by character. The first character in each name was the start token \\t. We'll feed the start token to the trained model to output a probability distribution over the vocabulary which can be sampled to generate the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2016/2016 [==============================] - 17s 9ms/step - loss: 1.1450\n",
      "Epoch 2/5\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 1.0045\n",
      "Epoch 3/5\n",
      "2016/2016 [==============================] - 15s 7ms/step - loss: 0.9647\n",
      "Epoch 4/5\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.9393\n",
      "Epoch 5/5\n",
      "2016/2016 [==============================] - 14s 7ms/step - loss: 0.9218\n"
     ]
    }
   ],
   "source": [
    "# Fit the model for 5 epochs using a batch size of 128 \n",
    "model.fit(input_data, target_data, batch_size=128, epochs=5)\n",
    "\n",
    "# Create a 3-D zero vector and initialize it with the start token\n",
    "output_seq = np.zeros((1, max_len+1, len(vocabulary)))\n",
    "output_seq[0, 0, char_to_idx['\\t']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0106 23:04:19.637348 11992 deprecation.py:323] From <ipython-input-13-63d352476d46>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n"
     ]
    }
   ],
   "source": [
    "probs = model.predict_proba(output_seq, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[5.05111893e-06, 1.40933131e-04, 8.54209065e-02, 4.80010808e-02,\n",
       "         8.72696936e-02, 6.19037561e-02, 6.96391016e-02, 2.32849047e-02,\n",
       "         3.89882736e-02, 3.22241895e-02, 1.82152372e-02, 7.73006305e-02,\n",
       "         3.65120210e-02, 8.26131925e-02, 8.02469328e-02, 2.15875525e-02,\n",
       "         1.79130249e-02, 2.16133650e-02, 2.31377152e-03, 5.74634708e-02,\n",
       "         5.02044857e-02, 3.63153443e-02, 2.14752415e-03, 1.88038312e-02,\n",
       "         2.04673074e-02, 3.96588439e-04, 2.52892659e-03, 4.95911064e-03,\n",
       "         1.90908868e-05, 5.47983545e-05, 5.10593200e-05, 7.32040326e-06,\n",
       "         8.39324275e-06, 2.51117817e-05, 1.14214854e-04, 8.11769351e-05,\n",
       "         8.36586114e-05, 9.77254385e-05, 2.88685515e-05, 7.13293703e-05,\n",
       "         2.83754889e-05, 3.29365685e-05, 5.61140187e-05, 1.56319758e-04,\n",
       "         5.09298879e-05, 4.78555849e-05, 8.55504004e-06, 2.60332981e-05,\n",
       "         5.41282825e-05, 2.60669876e-05, 1.38620264e-04, 3.24685607e-05,\n",
       "         1.91080180e-04, 2.75551829e-05],\n",
       "        [7.41022995e-06, 5.29358862e-04, 1.17851901e-04, 9.90898407e-05,\n",
       "         1.20020661e-04, 1.62850411e-04, 8.72691016e-05, 7.70141269e-05,\n",
       "         8.33229642e-05, 6.64390973e-05, 8.83352841e-05, 1.16792777e-04,\n",
       "         1.07943531e-04, 9.81736739e-05, 1.09242865e-04, 6.64849867e-05,\n",
       "         7.23943886e-05, 7.47506492e-05, 3.11992480e-05, 1.10546185e-04,\n",
       "         1.04984516e-04, 7.90059057e-05, 3.49936563e-05, 8.11532154e-05,\n",
       "         6.70688314e-05, 3.16388650e-05, 4.76981877e-05, 5.51156772e-05,\n",
       "         2.87795633e-01, 1.85439526e-03, 7.87603203e-04, 6.19071827e-04,\n",
       "         1.47231668e-01, 2.92595214e-04, 1.54598310e-04, 1.48100555e-02,\n",
       "         1.27019167e-01, 1.26569517e-04, 2.87087809e-04, 1.29838716e-02,\n",
       "         3.16271302e-03, 1.42107997e-03, 1.79781556e-01, 9.14728967e-04,\n",
       "         9.81631456e-05, 3.64420824e-02, 1.36314763e-03, 5.97167236e-04,\n",
       "         1.52111232e-01, 1.34334236e-03, 2.87267775e-03, 2.08864309e-04,\n",
       "         2.24103983e-02, 5.82315552e-04],\n",
       "        [4.99764724e-07, 1.17376927e-04, 1.23789607e-06, 1.55010537e-06,\n",
       "         1.02518857e-06, 1.95353846e-06, 1.00107536e-06, 9.86346777e-07,\n",
       "         1.17111244e-06, 1.04643993e-06, 8.85520876e-07, 1.18291644e-06,\n",
       "         1.27339888e-06, 1.07187896e-06, 1.20780135e-06, 5.61184549e-07,\n",
       "         1.00828242e-06, 7.97331040e-07, 6.31457624e-07, 1.04699495e-06,\n",
       "         1.30250430e-06, 4.86012254e-07, 7.07968923e-07, 1.44468629e-06,\n",
       "         1.57794636e-06, 7.47662227e-07, 8.90182946e-07, 6.98281553e-07,\n",
       "         1.36856422e-01, 9.10920929e-03, 9.59862210e-03, 1.05636995e-02,\n",
       "         2.39903450e-01, 1.61042728e-03, 4.15832689e-03, 4.85759083e-04,\n",
       "         2.01279148e-01, 2.45767966e-04, 1.01411960e-03, 2.67844461e-02,\n",
       "         2.45425235e-02, 1.64375622e-02, 6.81041181e-02, 2.08903337e-03,\n",
       "         7.58423703e-05, 4.61215265e-02, 2.57498007e-02, 2.11700406e-02,\n",
       "         4.33034934e-02, 7.53611466e-03, 1.42622227e-02, 1.54875941e-03,\n",
       "         8.29450861e-02, 4.35915682e-03],\n",
       "        [2.11414118e-07, 1.08830864e-02, 8.49985440e-07, 7.86671990e-07,\n",
       "         7.82146913e-07, 6.50302695e-07, 4.36657558e-07, 7.03732496e-07,\n",
       "         4.20505700e-07, 4.62033455e-07, 6.45633520e-07, 6.91782986e-07,\n",
       "         8.70827137e-07, 6.40584688e-07, 4.54287601e-07, 4.52950900e-07,\n",
       "         3.61829933e-07, 3.35952848e-07, 3.21030399e-07, 8.40269479e-07,\n",
       "         4.06056586e-07, 3.39186300e-07, 2.81263965e-07, 8.09938626e-07,\n",
       "         4.25386247e-07, 3.01093934e-07, 4.03379516e-07, 2.85992002e-07,\n",
       "         7.44047388e-02, 3.67305474e-03, 6.14686357e-03, 2.18465533e-02,\n",
       "         1.03822306e-01, 7.24735961e-04, 1.12619931e-02, 1.51010128e-04,\n",
       "         3.71766329e-01, 1.45486544e-03, 5.51719312e-03, 4.07670997e-02,\n",
       "         5.20955678e-03, 5.19389771e-02, 7.94702396e-02, 8.52657831e-04,\n",
       "         7.75258755e-04, 8.48949403e-02, 4.12569754e-02, 3.37500013e-02,\n",
       "         1.22994315e-02, 7.06257857e-03, 3.70421470e-03, 2.39386827e-05,\n",
       "         2.38488875e-02, 2.47847522e-03],\n",
       "        [5.32646638e-09, 3.15148011e-02, 1.04840545e-07, 5.88876681e-08,\n",
       "         8.75912036e-08, 1.26738442e-07, 7.91614951e-08, 5.50043602e-08,\n",
       "         7.40192050e-08, 1.02751287e-07, 1.07800687e-07, 1.19416811e-07,\n",
       "         7.16115949e-08, 7.78236213e-08, 1.07234790e-07, 4.96947656e-08,\n",
       "         3.38861064e-08, 4.42433361e-08, 3.00450225e-08, 1.05970713e-07,\n",
       "         9.52092876e-08, 6.82487027e-08, 2.14391456e-08, 8.44268868e-08,\n",
       "         6.50727046e-08, 1.68406213e-08, 3.48918832e-08, 3.22455342e-08,\n",
       "         5.38859665e-02, 1.08642725e-03, 3.53997061e-03, 6.19382481e-04,\n",
       "         3.06115657e-01, 1.32375761e-04, 1.07175758e-04, 1.62310331e-04,\n",
       "         3.66889775e-01, 7.31249529e-05, 4.12843656e-04, 2.07955744e-02,\n",
       "         2.71200715e-03, 3.54236178e-03, 1.37545085e-02, 1.21821591e-03,\n",
       "         4.96982502e-05, 1.18002584e-02, 3.84745859e-02, 8.94164667e-03,\n",
       "         1.96551508e-03, 4.21977544e-04, 6.60339196e-04, 5.38099766e-06,\n",
       "         1.30837172e-01, 2.79146014e-04],\n",
       "        [4.89019421e-08, 2.44468957e-01, 5.56343593e-09, 3.93354860e-09,\n",
       "         4.56690552e-09, 7.69056374e-09, 4.18307389e-09, 9.29146537e-09,\n",
       "         2.32072961e-09, 6.69637501e-09, 8.68998473e-09, 7.34935446e-09,\n",
       "         1.01913153e-08, 4.99725861e-09, 5.64384139e-09, 7.60438823e-09,\n",
       "         8.79179929e-09, 4.02958067e-09, 5.97622574e-09, 4.46107107e-09,\n",
       "         3.41888229e-09, 5.63748292e-09, 1.18142207e-08, 5.07006126e-09,\n",
       "         9.38880440e-09, 1.79437567e-08, 9.59837365e-09, 9.90015003e-09,\n",
       "         7.05972686e-02, 3.39199702e-04, 1.09198689e-03, 5.85499108e-02,\n",
       "         2.19799444e-01, 2.66387447e-04, 1.46244100e-04, 2.42420647e-04,\n",
       "         1.49199009e-01, 3.08712902e-06, 4.37354720e-05, 8.38506445e-02,\n",
       "         2.30161822e-03, 2.46426109e-02, 2.33290642e-02, 5.30967118e-05,\n",
       "         2.52032987e-05, 1.94041245e-02, 2.46068630e-02, 2.90065669e-02,\n",
       "         2.08752090e-03, 5.45835705e-04, 3.59352911e-04, 2.18247038e-07,\n",
       "         4.46991511e-02, 3.40528437e-04],\n",
       "        [4.73053774e-09, 8.26670647e-01, 1.57012565e-07, 8.33353155e-08,\n",
       "         1.03441039e-07, 2.12292974e-07, 1.52223592e-07, 1.59181539e-07,\n",
       "         6.85739536e-08, 1.67505689e-07, 6.61146871e-08, 2.42430247e-07,\n",
       "         1.38736326e-07, 6.39940723e-08, 1.83899900e-07, 8.14724217e-08,\n",
       "         6.49049525e-08, 5.43703536e-08, 3.59766581e-08, 1.41405465e-07,\n",
       "         1.31321585e-07, 2.46459621e-07, 5.26921262e-08, 6.19374632e-08,\n",
       "         1.60520003e-07, 4.41854198e-08, 5.47839321e-08, 3.56288012e-08,\n",
       "         1.73249841e-02, 2.62269517e-04, 3.67129629e-04, 3.36620281e-03,\n",
       "         3.22422162e-02, 9.39480196e-06, 3.15616562e-05, 1.01257399e-04,\n",
       "         3.07785776e-02, 6.07301627e-06, 2.68577651e-06, 1.11079235e-02,\n",
       "         3.02752736e-03, 3.20049301e-02, 6.20138645e-03, 1.29726759e-04,\n",
       "         8.75839760e-06, 3.60028865e-03, 2.50648893e-03, 5.23290690e-03,\n",
       "         7.19338842e-03, 7.34897767e-05, 9.40767815e-04, 3.10991936e-07,\n",
       "         1.66315753e-02, 1.74407018e-04],\n",
       "        [6.02470163e-09, 9.12179828e-01, 1.62304659e-08, 9.80490444e-09,\n",
       "         1.35924214e-08, 2.13865725e-08, 1.70434475e-08, 1.53923629e-08,\n",
       "         9.20860721e-09, 1.66329581e-08, 7.44772022e-09, 1.90610159e-08,\n",
       "         2.21468319e-08, 9.64334301e-09, 1.43608938e-08, 1.24623583e-08,\n",
       "         1.39237297e-08, 9.48381018e-09, 7.22064319e-09, 1.48741828e-08,\n",
       "         1.48277834e-08, 1.86509954e-08, 1.09331761e-08, 1.25682016e-08,\n",
       "         1.73225132e-08, 8.68222561e-09, 1.48823558e-08, 1.99763459e-08,\n",
       "         4.01366614e-02, 1.93674005e-05, 5.50113982e-05, 1.35591440e-03,\n",
       "         1.67561192e-02, 9.33752108e-06, 1.14186187e-05, 4.21694305e-04,\n",
       "         2.60306383e-03, 3.59295029e-07, 3.93834898e-06, 9.85453837e-04,\n",
       "         4.46268932e-05, 1.47901345e-02, 6.92273490e-03, 3.84619452e-05,\n",
       "         1.22767210e-06, 2.44786148e-04, 1.04749762e-03, 3.37354664e-04,\n",
       "         2.87473755e-04, 3.28795977e-06, 8.64719768e-05, 3.87624368e-08,\n",
       "         1.59501471e-03, 6.30637805e-05],\n",
       "        [2.90501889e-09, 9.69725192e-01, 6.43460467e-08, 4.10339815e-08,\n",
       "         3.47649625e-08, 1.12650298e-07, 6.22176941e-08, 4.64088146e-08,\n",
       "         4.37763639e-08, 7.70334339e-08, 1.42499426e-08, 5.46903536e-08,\n",
       "         6.06292616e-08, 3.46160860e-08, 3.18900746e-08, 2.86645907e-08,\n",
       "         3.09342809e-08, 2.60676671e-08, 1.18432624e-08, 4.18271888e-08,\n",
       "         4.79628817e-08, 6.37958735e-08, 2.38390498e-08, 4.17283772e-08,\n",
       "         8.12287695e-08, 1.43612677e-08, 4.85270490e-08, 4.73069122e-08,\n",
       "         4.79715131e-03, 2.19748881e-06, 2.10812095e-06, 8.44260285e-05,\n",
       "         8.06251075e-03, 7.96399320e-07, 4.26006136e-06, 2.68197153e-04,\n",
       "         4.43084682e-05, 1.07471571e-07, 8.74486716e-07, 7.53628992e-05,\n",
       "         2.84024263e-05, 9.78335552e-03, 5.45358798e-03, 9.20585251e-07,\n",
       "         1.70649628e-07, 1.31086737e-04, 2.47151649e-04, 4.79530107e-04,\n",
       "         2.13556006e-04, 3.46053099e-07, 1.22537922e-05, 1.74682224e-08,\n",
       "         5.77206956e-04, 4.15745762e-06],\n",
       "        [1.54902544e-08, 9.92131174e-01, 2.62330047e-07, 2.92151185e-07,\n",
       "         1.64056928e-07, 6.53866948e-07, 6.11286680e-07, 3.64409658e-07,\n",
       "         2.03153832e-07, 4.15589085e-07, 1.03871400e-07, 2.77246244e-07,\n",
       "         4.14355924e-07, 1.63927908e-07, 2.59536279e-07, 2.51502513e-07,\n",
       "         2.91911107e-07, 1.09290902e-07, 9.49733234e-08, 3.68601150e-07,\n",
       "         3.47555385e-07, 3.49500795e-07, 1.61081530e-07, 2.25272814e-07,\n",
       "         3.72999466e-07, 1.21321875e-07, 2.60648022e-07, 1.74587413e-07,\n",
       "         2.51667365e-03, 6.57339888e-06, 2.86729261e-07, 3.12061966e-05,\n",
       "         1.84624025e-03, 2.16051535e-06, 2.20431707e-06, 7.89345577e-05,\n",
       "         2.03246836e-05, 9.49146468e-08, 9.58197717e-08, 2.28858098e-05,\n",
       "         2.68532349e-05, 9.03410662e-04, 1.55372662e-03, 2.14077454e-06,\n",
       "         1.40283305e-07, 2.37122411e-04, 6.35467222e-05, 1.60431897e-04,\n",
       "         1.64122117e-04, 3.62815229e-07, 1.74293800e-05, 1.19578916e-08,\n",
       "         2.00170340e-04, 4.74942908e-06],\n",
       "        [1.02507033e-06, 9.87191200e-01, 9.41214694e-07, 1.40919497e-06,\n",
       "         4.21118330e-07, 2.62784897e-06, 2.33926426e-06, 1.14909005e-06,\n",
       "         1.13558576e-06, 2.03626200e-06, 4.55413044e-07, 8.97765631e-07,\n",
       "         2.08645406e-06, 6.74169485e-07, 1.42480155e-06, 1.69869213e-06,\n",
       "         1.98708631e-06, 3.98556665e-07, 1.12186569e-06, 1.45759964e-06,\n",
       "         1.75541129e-06, 1.51915583e-06, 1.15950161e-06, 1.83475106e-06,\n",
       "         1.91533377e-06, 1.16261413e-06, 1.94041127e-06, 1.55855196e-06,\n",
       "         7.57744536e-04, 4.15685145e-06, 1.37804307e-06, 3.89307424e-05,\n",
       "         8.32682848e-03, 1.31395836e-05, 5.58832880e-06, 5.90193958e-04,\n",
       "         1.21448747e-05, 4.07785649e-07, 2.67474820e-06, 1.11308282e-05,\n",
       "         9.61354635e-06, 8.74855614e-04, 5.97350881e-04, 3.01194868e-05,\n",
       "         1.39820713e-06, 5.17925422e-04, 6.20682375e-04, 1.67516730e-04,\n",
       "         4.22712947e-05, 1.88844137e-06, 4.51819033e-05, 3.54829297e-07,\n",
       "         8.24695308e-05, 1.45953954e-05],\n",
       "        [2.91258275e-05, 9.61407781e-01, 1.05271793e-05, 2.20994953e-05,\n",
       "         5.12573479e-06, 3.43557767e-05, 2.67766009e-05, 3.02906337e-05,\n",
       "         1.14808763e-05, 2.45088231e-05, 6.48828563e-06, 1.08120821e-05,\n",
       "         3.38824466e-05, 7.43796227e-06, 1.36216104e-05, 2.47578610e-05,\n",
       "         3.23199129e-05, 4.62971775e-06, 1.85390854e-05, 1.63776385e-05,\n",
       "         2.02632800e-05, 2.04299613e-05, 3.60131307e-05, 1.74135839e-05,\n",
       "         2.97204260e-05, 2.47651806e-05, 3.36038356e-05, 2.20999791e-05,\n",
       "         1.78351323e-03, 1.26088189e-05, 3.20322698e-07, 3.02029148e-05,\n",
       "         3.18459980e-02, 5.29074096e-05, 7.71744672e-05, 8.32455175e-04,\n",
       "         1.50676220e-04, 8.05166735e-07, 3.47927744e-06, 6.00444992e-06,\n",
       "         1.88792437e-05, 2.99892738e-04, 7.98364752e-04, 6.96061979e-05,\n",
       "         5.89311639e-06, 5.44662878e-04, 2.03135583e-04, 8.51714431e-05,\n",
       "         7.92529609e-05, 5.96139444e-06, 1.48721520e-04, 2.35545770e-07,\n",
       "         9.63922706e-04, 4.79858727e-06],\n",
       "        [3.23726745e-05, 9.89749253e-01, 6.99141019e-05, 1.94377062e-04,\n",
       "         8.83986722e-05, 2.93815916e-04, 2.81104876e-04, 1.57005328e-04,\n",
       "         1.14216513e-04, 2.06159108e-04, 4.81884235e-05, 1.29715016e-04,\n",
       "         1.66777842e-04, 4.56016533e-05, 1.11153735e-04, 1.44692240e-04,\n",
       "         1.54111229e-04, 4.76098467e-05, 1.49389380e-04, 1.43130004e-04,\n",
       "         3.15967016e-04, 1.47838728e-04, 1.82432195e-04, 1.73949564e-04,\n",
       "         1.69749968e-04, 2.04813987e-04, 1.46202510e-04, 1.03397317e-04,\n",
       "         1.12609356e-03, 1.50435826e-05, 1.35705264e-07, 7.42627662e-06,\n",
       "         2.12587509e-03, 6.58238514e-06, 1.38878122e-05, 7.47649901e-05,\n",
       "         2.05098579e-04, 1.60546078e-05, 7.10774520e-06, 6.62226967e-07,\n",
       "         4.09472295e-06, 3.38512546e-05, 7.89868529e-04, 1.60163745e-05,\n",
       "         2.29038028e-06, 6.92877802e-05, 2.84381422e-05, 1.37135321e-05,\n",
       "         8.64648828e-05, 1.26700377e-06, 5.57637759e-05, 1.51314083e-07,\n",
       "         1.52134674e-03, 7.41735812e-06]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probabilities for the first character\n",
    "probs = model.predict_proba(output_seq, verbose=0)[:,1,:] # 1st row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.41022995e-06, 5.29358862e-04, 1.17851901e-04, 9.90898407e-05,\n",
       "        1.20020661e-04, 1.62850411e-04, 8.72691016e-05, 7.70141269e-05,\n",
       "        8.33229642e-05, 6.64390973e-05, 8.83352841e-05, 1.16792777e-04,\n",
       "        1.07943531e-04, 9.81736739e-05, 1.09242865e-04, 6.64849867e-05,\n",
       "        7.23943886e-05, 7.47506492e-05, 3.11992480e-05, 1.10546185e-04,\n",
       "        1.04984516e-04, 7.90059057e-05, 3.49936563e-05, 8.11532154e-05,\n",
       "        6.70688314e-05, 3.16388650e-05, 4.76981877e-05, 5.51156772e-05,\n",
       "        2.87795633e-01, 1.85439526e-03, 7.87603203e-04, 6.19071827e-04,\n",
       "        1.47231668e-01, 2.92595214e-04, 1.54598310e-04, 1.48100555e-02,\n",
       "        1.27019167e-01, 1.26569517e-04, 2.87087809e-04, 1.29838716e-02,\n",
       "        3.16271302e-03, 1.42107997e-03, 1.79781556e-01, 9.14728967e-04,\n",
       "        9.81631456e-05, 3.64420824e-02, 1.36314763e-03, 5.97167236e-04,\n",
       "        1.52111232e-01, 1.34334236e-03, 2.87267775e-03, 2.08864309e-04,\n",
       "        2.24103983e-02, 5.82315552e-04]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.random.choice\n",
    "\n",
    "```\n",
    " numpy.random.choice(a, size=None, replace=True, p=None)¶\n",
    " \n",
    "Parameters\n",
    "\n",
    "    a :1-D array-like or int\n",
    "        If an ndarray, a random sample is generated from its elements. If an int, the random sample is generated as if a were np.arange(a) sizeint or tuple of ints, optional\n",
    "\n",
    "Output shape : If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.\n",
    "    replace: boolean, optional\n",
    "\n",
    "        Whether the sample is with or without replacement\n",
    "    p : 1-D array-like, optional\n",
    "\n",
    "        The probabilities associated with each entry in a. If not given the sample assumes a uniform distribution over all entries in a.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# Sample vocabulary to get first character\n",
    "first_char = np.random.choice(sorted(vocabulary), replace=False, p=probs.reshape(len(vocabulary)))\n",
    "\n",
    "# Print the character genaerated\n",
    "print(first_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we know how to train the RNN model and generate the first character given the seed character as input. We'll use this character to generate the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate baby names\n",
    "- generate the second character by feeding the start token and the generated first character again to the trained network. We'll also be generating full names starting from the start token and repeating this process until the end token is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "# Print the first character which we got last time\n",
    "print(first_char)\n",
    "\n",
    "# Update the vector to contain first the character\n",
    "output_seq[0, 1, char_to_idx[first_char]] = 1\n",
    "\n",
    "# Get the probabilities for the second character\n",
    "probs = model.predict_proba(output_seq, verbose=0)[:,2,:]\n",
    "\n",
    "# Sample vocabulary to get second character\n",
    "second_char = np.random.choice(sorted(vocabulary), replace=False, p=probs.reshape(len(vocabulary)))\n",
    "\n",
    "# Print the second character\n",
    "print(second_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_baby_names(n):\n",
    "    for i in range(0,n):\n",
    "        stop=False\n",
    "        counter=1\n",
    "        name=''\n",
    "        # initialize first char of output seq\n",
    "        output_seq=np.zeros((1,max_len+1,len(vocabulary)))\n",
    "        output_seq[0,0,char_to_idx['\\t']] = 1\n",
    "        # continue until a newline is generated or max no of chars reached\n",
    "        while stop==False and counter<10:\n",
    "            # get prob distribution for next char\n",
    "            probs = model.predict_proba(output_seq, verbose=0)[:,counter-1,:]\n",
    "            # sample vocab to get most probable next char\n",
    "            c = np.random.choice(sorted(list(vocabulary)), replace=False, p=probs.reshape(len(vocabulary)))\n",
    "            if c=='\\n':\n",
    "                stop=True\n",
    "            else:\n",
    "                name = name + c\n",
    "                output_seq[0,counter,char_to_idx[c]] = 1\n",
    "                counter+= 1\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daciey\n",
      "Corra\n",
      "Zannart\n",
      "Ulbith\n",
      "Jeathin\n"
     ]
    }
   ],
   "source": [
    "generate_baby_names(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple network using Keras\n",
    "- We know how the gradient values become lesser and lesser as we back-propagate.\n",
    "- demonstrate this vanishing gradient problem. We'll create a **simple network of Dense layers using Keras and checkout the gradient values of the weights for one iteration of back-propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0106 23:11:22.467451 23368 deprecation.py:506] From C:\\Users\\Shubham\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# The TensorFlow 2.0 has enabled eager execution by default. At the starting of algorithm, you need to use tf.compat.v1.disable_eager_execution() to disable eager execution. \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Create a dense layer of 12 units\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# Create a dense layer of 8 units\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# Create a dense layer of 1 unit\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile the model and get gradients\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\")\n",
    "with tf.GradientTape() as gtape:\n",
    "    gradients = gtape.gradient(model.output, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we have a toy network to test gradient values of weights from each layer. Checking gradient values from successive layers is crucial to identify vanishing or exploding gradient problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients\n",
    "- Before we start working on more robust applications of language generation, it's best to learn how to identify when you are suffering from the vanishing gradient problem.\n",
    "- We will train the network created using random training data. \n",
    "- To run a model in `Tensorflow`, we need to initialize a tensorflow session first by using the `InteractiveSession()` function. Then we will initialize all variables using the `global_variables_initializer()` function.\n",
    "- We will execute gradients node which we created inside this tensorflow session. We will also check some of the gradient values from different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-517b886dc91e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Evaluate the gradients using the training examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mevaluated_gradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Print gradient values from third layer and two nodes of the second layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 958\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1164\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1166\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    475\u001b[0m     \"\"\"\n\u001b[0;32m    476\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections_abc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' %\n\u001b[1;32m--> 263\u001b[1;33m                       (fetch, type(fetch)))\n\u001b[0m\u001b[0;32m    264\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Create a dummy input vector\n",
    "input_vector = np.random.random((1,8))\n",
    "\n",
    "# Create a tensorflow session to run the network\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "# Initialize all the variables\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "# Evaluate the gradients using the training examples\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:input_vector})\n",
    "\n",
    "# Print gradient values from third layer and two nodes of the second layer\n",
    "print(evaluated_gradients[4])\n",
    "print(evaluated_gradients[2][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
